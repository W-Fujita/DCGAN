{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DCGAN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ubik1xxpo_L8","colab_type":"code","colab":{}},"source":["#GPUを認識しているか確認\n","import tensorflow as tf\n","tf.test.gpu_device_name()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FeSt6IrUpFFb","colab_type":"code","colab":{}},"source":["#ファイルの読み書きを可能にする\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","#ファイルを置いてあるパスを入力\n","file_path = \"/content/drive/My Drive/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F7smbTbbpHyS","colab_type":"code","colab":{}},"source":["import keras\n","from keras.layers import Input, Dense, Activation, BatchNormalization, Reshape, UpSampling2D, Conv2D, MaxPool2D, Flatten\n","from keras.models import Model\n","from keras.optimizers import Adam\n","import numpy as np\n","\n","#画像生成器\n","class Generator():\n","    def __init__(self, z_dim):\n","        inputs = Input(shape=(z_dim,))\n","        x = Dense(1024)(inputs)\n","        x = Activation(\"tanh\")(x)\n","        x = Dense(128*7*7)(x)\n","        x = BatchNormalization()(x)\n","        x = Activation(\"tanh\")(x)\n","        x = Reshape((7, 7, 128))(x)\n","        x = UpSampling2D(size=(2, 2))(x)\n","        x = Conv2D(64, 5, padding=\"same\")(x)\n","        x = Activation(\"tanh\")(x)\n","        x = UpSampling2D(size=(2, 2))(x)\n","        x = Conv2D(1, 5, padding=\"same\")(x)\n","        predictions = Activation(\"tanh\")(x)\n","        self.generator = Model(inputs, predictions)\n","\n","    def get_model(self):\n","        return self.generator\n","\n","\n","#画像判別器\n","class Discriminator():\n","    def __init__(self, height, width, channels):\n","        inputs = Input(shape=(height, width, channels))\n","        x = Conv2D(64, 5, padding=\"same\")(inputs)\n","        x = Activation(\"tanh\")(x)\n","        x = MaxPool2D()(x)\n","        x = Conv2D(128, 5)(x)\n","        x = Activation(\"tanh\")(x)\n","        x = MaxPool2D()(x)\n","        x = Flatten()(x)\n","        x = Dense(1024)(x)\n","        x = Activation(\"tanh\")(x)\n","        x = Dense(1)(x)\n","        predictions = Activation(\"sigmoid\")(x)\n","        self.discriminator = Model(inputs, predictions)\n","\n","    def get_model(self):\n","        return self.discriminator\n","\n","\n","class DCGAN():\n","    def __init__(self, z_dim, height, width, channels):\n","        self.z_dim = z_dim\n","        #生成器を作成\n","        g = Generator(z_dim)\n","        self.generator = g.get_model()\n","        #判別器を作成\n","        d = Discriminator(height, width, channels)\n","        self.discriminator = d.get_model()\n","        #判別器をコンパイル\n","        discriminator_optimizer = Adam(lr=0.0002, beta_1=0.5)\n","        self.discriminator.compile(loss=\"binary_crossentropy\", optimizer=discriminator_optimizer)\n","        #判別器のパラメータを固定\n","        self.discriminator.trainable = False\n","        #DCGANを作成\n","        inputs = Input(shape=(z_dim,))\n","        outputs = self.discriminator(self.generator(inputs))\n","        self.dcgan = Model(inputs, outputs)\n","        #DCGANをコンパイル\n","        dcgan_optimizer = Adam(lr=0.0002, beta_1=0.5)\n","        self.dcgan.compile(loss=\"binary_crossentropy\", optimizer=dcgan_optimizer)\n","\n","    def train(self, real_images, batch_size):\n","        #判別器が偽物を検出できるように訓練\n","        noise = np.random.uniform(-1, 1, size=(batch_size, self.z_dim))\n","        generated_images = self.generator.predict(noise)\n","        labels = np.zeros((batch_size, 1))\n","        d_loss_fake = self.discriminator.train_on_batch(generated_images, labels)\n","        #判別器が本物を検出できるように訓練\n","        labels = np.ones((batch_size, 1))\n","        d_loss_real = self.discriminator.train_on_batch(real_images, labels)\n","        d_loss = np.add(d_loss_fake, d_loss_real) / 2.0\n","        #生成器が判別器をだますことができるように訓練\n","        labels = np.ones((batch_size, 1))\n","        g_loss = self.dcgan.train_on_batch(noise, labels)\n","        return d_loss, g_loss\n","\n","    #noiseに対する予測値を生成\n","    def predict(self, noise):\n","        return self.generator.predict(noise)\n","\n","    #学習結果の読み込み\n","    def load_weights(self, file_path, by_name=False):\n","        self.dcgan.load_weights(file_path, by_name)\n","\n","    #学習結果の保存\n","    def save_weights(self, file_path, overwrite=True):\n","        self.dcgan.save_weights(file_path, overwrite)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qTGDid1CpP6k","colab_type":"code","colab":{}},"source":["import os\n","import keras\n","from keras.datasets import mnist\n","from keras.preprocessing import image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","#ピクセル値を-1から1に正規化\n","def normalize(X):\n","    return (X-127.5) / 127.5\n","\n","#ピクセル値を0から255に戻す\n","def denormalize(X):\n","    return (X+1.0) * 127.5\n","\n","def train(z_dim, height, width, channels, epochs):\n","    os.makedirs(file_path+\"Model\", exist_ok=True) #Modelフォルダの作成\n","    dcgan = DCGAN(z_dim, height, width, channels)\n","    (x_train, y_train), (_, _) = mnist.load_data() #60000枚の訓練画像データ\n","    x_train = x_train.reshape(x_train.shape[0], height, width, channels).astype(\"float32\")\n","    x_train = normalize(x_train)\n","    batch_size = 128\n","    iterations = x_train.shape[0] // batch_size #iterations=468\n","    save_interval = 12 #irerationsの任意の約数\n","    noise = np.random.uniform(-1, 1, size=(64, z_dim)) #-1から1の一様乱数\n","    for epoch in range(epochs):\n","        print(\"epoch{} start\".format(str(epoch)))\n","        for iteration in range(iterations):\n","            real_images = x_train[iteration*batch_size:(iteration+1)*batch_size]\n","            d_loss, g_loss = dcgan.train(real_images, batch_size)\n","            #損失値の保存\n","            if epoch == 0 and iteration == 0:\n","                with open(file_path+\"Loss.txt\", \"a\") as f:\n","                    f.write(str(d_loss) + \",\" + str(g_loss) + \"\\n\")\n","            elif (iteration+1) % save_interval == 0:\n","                with open(file_path+\"Loss.txt\", \"a\") as f:\n","                    f.write(str(d_loss) + \",\" + str(g_loss) + \"\\n\")\n","        print(\"epoch{} end\".format(str(epoch)))\n","        dcgan.save_weights(file_path+\"Model/dcgan_epoch{}.h5\".format(str(epoch)))\n","        predict(z_dim, height, width, channels, epoch, noise)\n","\n","def predict(z_dim, height, width, channels, epoch, noise):\n","    os.makedirs(file_path+\"Generated\", exist_ok=True) #generatedフォルダの作成\n","    dcgan = DCGAN(z_dim, height, width, channels)\n","    dcgan.load_weights(file_path+\"Model/dcgan_epoch{}.h5\".format(str(epoch)))\n","    generated_images = dcgan.predict(noise)\n","    images = [] #imagesの初期化\n","    for generated_image in generated_images:\n","        images.append(image.array_to_img(denormalize(generated_image), scale=False)) #ndarrayからPIL形式に変換\n","    save_image(images, epoch)\n","\n","def save_image(images, epoch):\n","    H = 8 #縦に並べる個数\n","    W = 8 #横に並べる個数\n","    fig = plt.figure(figsize=(H, W))\n","    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0, wspace=0)\n","    for i,image in enumerate(images):\n","        ax = fig.add_subplot(H, W, i+1, xticks=[], yticks=[])\n","        ax.imshow(image, cmap=\"gray\")\n","    plt.savefig(file_path+\"Generated/{}.jpg\".format(epoch))\n","    plt.close()\n","\n","def create_gif(epochs):\n","    images = []\n","    for epoch in range(epochs):\n","        images.append(Image.open(file_path+\"Generated/{}.jpg\".format(epoch)))\n","    images[0].save(file_path+\"DCGAN.gif\", save_all=True, append_images=images[1:], optimize=False, duration=200, loop=0)\n","\n","def display_loss(epochs):\n","    #損失値の取り出し\n","    d_loss = []\n","    g_loss = []\n","    with open(file_path+\"Loss.txt\", \"r\") as f:\n","        for line in f:\n","            data = line.split(\",\")\n","            d_loss.append(float(data[0]))\n","            g_loss.append(float(data[1]))\n","    os.remove(file_path+\"Loss.txt\")\n","    #損失値の表示\n","    x = np.linspace(0, (len(d_loss)-1)*12, len(d_loss))\n","    plt.plot(x, d_loss)\n","    plt.plot(x, g_loss)\n","    plt.title(\"Discriminator loss vs Generator loss\")\n","    plt.ylabel(\"loss\")\n","    plt.xlabel(\"iteration\")\n","    plt.xticks(np.linspace(0, (len(d_loss)-1)*12, int(epochs/2)+1)) #x軸のメモリの設定\n","    plt.grid()\n","    plt.legend([\"D_loss\", \"G_loss\"], loc=\"best\")\n","    plt.savefig(file_path+\"Loss.jpg\")\n","    plt.show()\n","\n","def main():\n","    z_dim = 100 #潜在変数の次元数\n","    height = 28 #高さ28\n","    width = 28 #幅28\n","    channels = 1 #グレースケール画像\n","    epochs = 20 #学習させる回数\n","    train(z_dim, height, width, channels, epochs)\n","    create_gif(epochs)\n","    display_loss(epochs)\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":0,"outputs":[]}]}